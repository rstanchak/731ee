\documentclass[letter,11pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{eurosym}
\usepackage[english]{babel}
\usepackage{verbatim}
\bibliographystyle{plain}


\title{ENEE 731: Image Understanding (Fall 2006)\\
End-Semester Exam}
\author{Roman Stanchak}
\date{\today}


\begin{document}
\maketitle

\section*{Problem 1}

	{\begin{proof} If the distance between camera centers is $2s$, then the locations of the camera centers $C_l,C_r$ are $[-s, 0, 0]$ and $[s, 0, 0]$.  Assuming the camera viewing axes are parallel with the z axis of the 3-D point $\mathbf{X}$ in the canonical coordinate system is therefore related to points $\mathbf{X_l}$ and $\mathbf{X_r}$ in the camera coordinate systems by:
$$\mathbf{X_l}=\mathbf{X}-C_l$$
$$\mathbf{X_r}=\mathbf{X}-C_r$$
Since only the $X$ coordinate is affected, $Y_l=Y_r, Z_l=Z_r$ and these equation reduce to:
$$X_l=X+s$$
$$X_r=X-s$$
Assuming a perspective projection with focal length $f$, points in the images ($\mathbf{x_l},\mathbf{x_r}$) are related to points in the camera view by:
$$X_l = \frac{x_l Z}{f}$$
$$Y_l = \frac{y_l Z}{f}$$
and similarly for $\mathbf{X_r}$ and $\mathbf{x_r}$.\\
Substituting for $X_l,X_r$ in the equations above:
$$\frac{x_l Z}{f} = X + s$$
$$\frac{x_r Z}{f} = X - s$$
Subtracting the bottom from the top equation:
$$\frac{Z}{f}(x_l-x_r) = 2s$$
Solving for $Z$:
$$Z = \frac{2sf}{x_l-x_r}$$
For how small changes in $(x_l-x_r)$ affect $Z$, take derivative WRT $(x_l-x_r)$:
\begin{equation*}
	\frac{dZ}{d(x_l-x_r)} = -\frac{2sf}{(x_l-x_r)^2}
\end{equation*}
Replace $\frac{2sf}{x_l-x_r}$ with $Z$ on the RHS, then rearrange:
\begin{eqnarray*}
	\frac{dZ}{d(x_l-x_r)} &=& -\frac{Z}{(x_l-x_r)}\\
	\frac{dZ}{Z} &=& -\frac{d(x_l-x_r)}{(x_l-x_r)}
\end{eqnarray*}
\end{proof}


\section*{Problem 2}

	{\begin{proof}[Proof of (a)]
Given:
$$Z=Z_0 + pX + qY$$\\
Assuming a perspective projection with focal length $f$: \\
$$X=\frac{xZ}{f},Y=\frac{yZ}{f}$$\\
Substituting:\\
$$Z = Z_0 + \frac{pxZ}{f}+\frac{qyZ}{f}$$\\
Shifting terms:\\
$$Z_0 = \frac{Z}{f} \cdot \left(1 - px - qy)\right)$$\\
Assuming $f=1$, :\\
$$\frac{Z_0}{Z}=1-px-qy$$
\end{proof}}
\begin{proof}[Proof of (b)]
	Given the equations for the translational and rotation components of optic flow\cite{robotvision}:
\begin{eqnarray*}
	u_t = \frac{-U+xW}{Z} \textrm{ and } u_r = Axy - B(x^2+1) + Cy,\\
	v_t = \frac{-V+yW}{Z} \textrm{ and } v_r = A(y^2+1) - Bxy - Cy.
\end{eqnarray*}
From the result above, we have:
\begin{equation}
	\frac{1}{Z} = \frac{1}{Z_0}(1-px-qy)
\end{equation}
Substituting this in the optic flow equations:
\begin{eqnarray*}
	u = u_t + u_r = \frac{1}{Z_0}(-U+xW)(1-px-qy) + Axy - B(x^2+1) + Cy\\
	v = v_t + v_r = \frac{1}{Z_0}(-V+yW)(1-px-qy) + A(y^2+1) - Bxy - Cy
\end{eqnarray*}
Substituting $U'=U/Z_0,V'=V/Z_0,W'=W/Z_0$:
\begin{eqnarray*}
    u = (-U'+xW')(1-px-qy) + Axy - B(x^2+1) + Cy\\
    v = (-V'+yW')(1-px-qy) + A(y^2+1) - Bxy - Cy
\end{eqnarray*}
\end{proof}
\begin{proof}[Proof of (c)]
If two different planes moving under different motions exhibit the same motion parameters, then:
\begin{eqnarray*}
	    u_1 = u_2, \forall (x,y) \in I \\
		v_1 = v_2, \forall (x,y) \in I
\end{eqnarray*}
Thus, for $u$:
\begin{equation*}
	\begin{split}
	(-U_1'+xW_1')(1-p_1x-q_1y) + A_1xy - B_1(x^2+1) + C_1y = \\
	 (-U_2'+xW_2')(1-p_2x-q_2y) + A_2xy - B_2(x^2+1) + C_2y 
\end{split}
\end{equation*}
Setting coefficients for like terms equal:
$$\begin{array}{llcl}
	x^2 & W_2' p_{2} + B_{2} =  W_1' p_{1} + B_{1}  &=> &   W_1'p_1-W_2'p_2=B_2-B_1 \\
	xy  & A_{1} - W_1' q_{1} = A_{2} - W_2' q_{2}   &=> &   W_1'q_1-W_2'q_2=A_1-A_2 \\
	x   & U_1' p_{1} + W_1' = U_{2} p_{2} + W_2'   &=> &   U_1'p_1-U_2p_2=W_2'-W_1' \\
	y   & C_{1} + U_1' q_{1} = C_{2} + U_{2}' q_{2}  & => &   U_1'q_1-U_2q_2=C_2-C_1 \\
	1   & U_2' + B_{2} = U_1' + B_{1}               & => &   U_1' - U_2' = B_2-B_1 
\end{array}
$$
Equivalently for $v$:
\begin{equation*}
	\begin{split}
		(-V_1'+xW_1')(1-p_1x-q_1y) + A_1(y^2+1) - B_1xy + C_1y = \\
		(-V_2'+xW_2')(1-p_2x-q_2y) + A_2(y^2+1) - B_2xy + C_2y 
	\end{split}
\end{equation*}
Again setting coefficients for like terms equal:
$$\begin{array}{llcl}
	y^2 & A_{1} - W_{1}' q_{1} = A_{2} - W_{2}' q_{2}  &=> & W_1'q_1 - W_2'q_2 = A_1-A_2 \\ 
	xy  &  W_{1}' p_{1} + B_{1} = W_{2}' p_{2} + B_{2}   &=> & W_1'p_1 - W_2'p_2 = B_2-B_1 \\
	x   &  C_{1} + V_{1}' p_{1} =  C_{2} + V_{2}' p_{2}   &=> & V_1'p_1 - V_2'p_2 = C_2-C_1 \\
	y   & V_{1}' q_{1} + W_{1}' = V_{2}' q_{2} + W_{2}'  & => & V_1'q_1 - V_2'q_2 = W_2'-W_1' \\ 
	1   & A_{1} - V_{1}' = A_{2} - V_{2}'               & => &   V_1' - V_2' = A_1-A_2 
\end{array}
$$
Two of these repeat, so we are left with the following 8 conditions:
\begin{eqnarray*}
	V_1' - V_2' &=& A_1-A_2\\
	W_1'q_1 - W_2'q_2 &=& A_1-A_2 \\
	U_1' - U_2' &=& B_2-B_1 \\
	W_1'p_1 - W_2'p_2 &=& B_2-B_1 \\ 
	U_1'q_1-U_2q_2 &=& C_2-C_1 \\
	V_1'p_1 - V_2'p_2 &=& C_2-C_1 \\
	U_1'p_1-U_2p_2 &=& W_2'-W_1' \\
	V_1'q_1 - V_2'q_2 &=& W_2'-W_1'
\end{eqnarray*}
\end{proof}
Note the ambiguity between the plane offset $Z_0$ and the translational components $U,V,W$ as 
indicated by the prime terms.  For any given $U',V',W'$, the same 2d motion would be observed for planes with identical ($p,q$), but any offset $Z_0$ with the $3D$ translation $Z_0\cdot[U,V,W]$, and zero rotation.

\section*{Problem 3}
The integral of the square of the difference between the observed and predicted brightness is:
\begin{equation*}
	\int\int_I \left( E(x,y) - \mathbf{n}(x,y)\cdot\mathbf{s}\right)^2 dxdy
\end{equation*}
In order to minimize this integral with respect to the direction of illumination $\mathbf{s}$, take
the derivative with respect to $\mathbf{s}$ and set equal to zero.
\begin{eqnarray*}
	\frac{d}{d\mathbf{s}}\int\int_I \left( E - \mathbf{n}\cdot\mathbf{s}\right)^2 dxdy &=& 0\\
	\int\int_I -2\mathbf{n}\left( E - \mathbf{n}\cdot\mathbf{s}\right) dxdy &=& 0\\
	\int\int_I E{n}dxdy - \int\int_I\mathbf{n}\mathbf{n}\cdot\mathbf{s}dxdy &=& 0\\
\end{eqnarray*}
Using the fact that given two vectors $\mathbf{x},\mathbf{y}$, $xy^T=x\cdot y$, and $\mathbf{s}$ 
is independent of the integration:
\begin{eqnarray*}
	\int\int_I E\mathbf{n}dxdy &=& \int\int_I \mathbf{n}(\mathbf{n}^T\mathbf{s}) dxdy \\
	\int\int_I E\mathbf{n}dxdy &=& \left[\int\int_I \mathbf{n}\mathbf{n}^T dxdy\right]\mathbf{s}
\end{eqnarray*}
Thus:
\begin{equation*}
	\mathbf{s} = \left[\int\int_I \mathbf{n}\mathbf{n}^T dxdy\right]^{-1}\int\int_I E\mathbf{n}dxdy
\end{equation*}
Discretizing,
\begin{equation*}
	\mathbf{s} = \left[\sum_x\sum_y \mathbf{n}(x,y)\mathbf{n}(x,y)^T \right]^{-1}\sum_x\sum_y E(x,y)\mathbf{n}(x,y)dxdy
\end{equation*}
In order for the inverse to exist, the matrix produced by the sum $\sum_x\sum_y \mathbf{n}\mathbf{n}^T$ must be non-singular.  In order for this to be the case, the sum must be
over three or more linearly independent $\mathbf{n}$ vectors.  This is the case because the rows of a single $\mathbf{n}\mathbf{n}^T$ are simply scaled versions of the vector $\mathbf{n}$.  Thus, the rows for any sum of $m$ $\mathbf{n}\mathbf{n}^T$ are each a linear combination of the $m$ $\mathbf{n}$ vectors.  Therefore, in order for the resulting rows to be linearly independent, the must consist of a sum of three linearly independent $\mathbf{n}$ vectors.

If the surface's brightness is proportional to $\sqrt{cos \theta_i/cos\theta_j}$, the integral becomes the following: \textbf{TODO}

\section*{Problem 4}
{
Show the Euler functions for the error function:
$$\int\int\left((E(x,y) - R(p,q))^2 + \lambda(p_y - q_x)^2\right)dxdy$$
are:
\begin{eqnarray*}
	(E(x,y) - R(p,q))R_p &=& \lambda(p_{yy}-q_{xy})\\
	(E(x,y) - R(p,q))R_q &=& \lambda(q_{xx}-p_{xy}) 
\end{eqnarray*}
\begin{proof}
% see Horn & Brooks (1986) p. 204 for minimizing functionals using Euler equations
The functional we seek to minimize is parameterized in terms of surface normals $p$ and $q$, and takes the general form:
$$I(p,q) = \int\int F(x,y,p,q,p_x,p_y,q_x,q_y)dxdy$$
The Euler equations for this general form are as follows \cite{horn1986}:
\begin{eqnarray*}
	F_p - \frac{\partial}{\partial x}F_{p_x}-\frac{\partial}{\partial y}F_{p_y} &=& 0 \\
	F_q - \frac{\partial}{\partial x}F_{q_x}-\frac{\partial}{\partial y}F_{q_y} &=& 0 
\end{eqnarray*}
The partials for the given functional are as follows:
\begin{eqnarray*}
	F &=& (E(x,y) - R(p,q))^2 + \lambda(p_y - q_x)^2\\
	F_p &=& 2R_p(E(x,y)-R(p,q)) \\
	F_q &=& 2R_q(E(x,y)-R(p,q)) \\
	F_{p_x} &=& 0 \\
	F_{p_y} &=& 2\lambda(p_y-q_x) \\
	F_{q_x} &=& -2\lambda(p_y-q_x) \\
	F_{q_y} &=& 0 \\
\end{eqnarray*}
The derivation of the first Euler function is:
\begin{eqnarray*}
    F_p - \frac{\partial}{\partial x}F_{p_x}-\frac{\partial}{\partial y}F_{p_y} &=& 0 \\
	2R_p(E(x,y)-R(p,q)) - 0 - 2\lambda\frac{\partial}{\partial y}(p_y-q_x) &=& 0 \\
	2R_p(E(x,y)-R(p,q)) - 2\lambda(p_{yy}-q_{xy}) &=& 0 \\
	(E(x,y)-R(p,q))R_p &=& \lambda(p_{yy}-q_{xy})
\end{eqnarray*}
The derivation of the second is:
\begin{eqnarray*}
    F_q - \frac{\partial}{\partial x}F_{q_x}-\frac{\partial}{\partial y}F_{q_y} &=& 0 \\
	2R_q(E(x,y)-R(p,q)) + 2\lambda\frac{\partial}{\partial x}(p_y-q_x) - 0 &=& 0 \\
    2R_q(E(x,y)-R(p,q)) + 2\lambda(p_{xy}-q_{xx}) &=& 0 \\
    (E(x,y)-R(p,q))R_p &=& \lambda(q_{xx}-p_{xy})
\end{eqnarray*}
\end{proof}
}
{
An iterative scheme based on the isolation of the central values in discrete approximations of the second order derivatives $p_{yy}$ and $q_{xx}$ is given below:
The finite difference approximations for the second partial derivatives of a 2D function $f$ are as follows:
\begin{eqnarray*}
	F_{xx} &=& \bar{F_x}-2F(x,y) \\ 
	F_{yy} &=& \bar{F_y}-2F(x,y) \\
	F_{xy} &=& \bar{F}-4F(x,y) \\
	\textrm{ where } & &\\
	\bar{F_x}&=&F(x+1,y)+F(x-1,y) \\
	\bar{F_y}&=&F(x,y+1)+F(x,y-1) \\
	\bar{F}&=&F(x+1,y)+F(x-1,y)+f(x,y+1)+F(x,y-1)
\end{eqnarray*}
Substituting these for the partials the first Euler equation above and isolating $p(x,y),q(x,y)$ results in:
\begin{eqnarray*}
	(E(x,y)-R(p,q))R_p &=& 
		\lambda ( \bar{p_y}(x,y) - 2p(x,y) - \left( \bar{q}(x,y) - 4q(x,y)\right))\\
	\lambda\left( 2p(x,y) - 4q(x,y)\right) &=& 
		-(E(x,y)-R(p,q))R_p + \lambda ( \bar{p_y}(x,y) - \bar{q}(x,y)
\end{eqnarray*}
And for the second:
\begin{eqnarray*}
	(E(x,y)-R(p,q))R_q &=&
		\lambda ( \bar{q_x(x,y)} - 2q(x,y)  - \left( \bar{p}(x,y) - 4p(x,y)\right))\\
	\lambda\left( -4p(x,y) + 2q(x,y)\right) &=&
		-(E(x,y)-R(p,q))R_q + \lambda ( \bar{q_x}(x,y) - \bar{p}(x,y)  
\end{eqnarray*}
These two constraints can be used in an iterative solution to $p,q$ by using an intial estimate on the right side of the equation to compute new estimates on the left.  Let $p^{(k)}$ represent the estimate of $p$ at iteration $k$, and likewise for $q^{(k)}$. 
\begin{align*}
	\begin{split}
	\lambda\left( 2p^{(k+1)}(x,y) - 4q^{(k+1)}(x,y)\right) = &
     	-\left(E(x,y)-R(p^{(k)},q^{(k)}))R_p \ + \right.\\
	& \left.\lambda ( \bar{p_y}^{(k)}(x,y) - \bar{q}^{(k)}(x,y) )\right)
	\end{split}\\
	\begin{split}
    \lambda\left( -4p^{(k+1)}(x,y) + 2q^{(k+1)}(x,y)\right) = &
        -\left(E(x,y)-R(p^{(k)},q^{(k)}))R_q \ + \right.\\
		& \left. \lambda ( \bar{q_x}^{(k)}(x,y) - \bar{p}^{(k)}(x,y) ) \right)
	\end{split}
\end{align*}
By taking a linear combination of these, $q^{(k+1)}$ is eliminated to get:
\begin{align*}
	\begin{split}
	-6\lambda p^{(k+1)}(x,y) = & 
	 -\left(E(x,y)-R(p^{(k)},q^{(k)})\right)R_p + \lambda ( \bar{p_y}^{(k)}(x,y) - \bar{q}^{(k)}(x,y)) \ + \\
      & 2\left[  -\left(E(x,y)-R(p^{(k)},q^{(k)})\right)R_q + \lambda ( \bar{q_x}^{(k)}(x,y) - \bar{p}^{(k)}(x,y))\right]
  \end{split}\\
  \begin{split}
	  p^{(k+1)}(x,y) = & - (R_p + 2 R_q)\left(E(x,y)-R(p^{(k)},q^{(k)})\right) \\ 
	  & + \lambda\left( \bar{p_y}^{(k)} - \bar{q}^{(k)} + 2\bar{q_x}^{(k)} -       2\bar{p}^{(k)}\right)
  \end{split}
\end{align*}
And likewise to eliminate $p^{(k)}$:
\begin{align*}
	\begin{split}
		-6\lambda q^{(k+1)}(x,y) = & 
		2\left[ -\left(E(x,y)-R(p^{(k)},q^{(k)})\right)R_p + \lambda ( \bar{p_y}^{(k)}(x,y) - \bar{q}^{(k)}(x,  y)) \ \right]+ \\
		& -\left(E(x,y)-R(p^{(k)},q^{(k)})\right)R_q + \lambda ( \bar{q_x}^{(k)}(x,y) -       \bar{p}^{(k)}(x,y))
	\end{split}\\
	\begin{split}
		q^{(k+1)}(x,y) = & - (2 R_p + R_q)\left(E(x,y)-R(p^{(k)},q^{(k)})\right)\\ 
		& + \lambda\left( 2\bar{p_y}^{(k)} - 2\bar{q}^{(k)} + \bar{q_x}^{(k)} - \bar{p}^{(k)}\right)
	\end{split}
\end{align*}


Thus, given initial estimates $p^{(0)}, q^{(0)}$, these update equations can be used to iteratively 
solve for $p,q$ that minimize the integrability constraint.

\section*{Problem 5}
Given $n$ pairs of corresponding points $p_1, p_2, \dots p_i, \dots p_n$ and $q_1, q_2, \dots q_i, \dots q_n$, we want to find the 3-D displacement $(R,t)$ that minimizes the sum of squared distances:
\begin{eqnarray*}
	\mathcal{C}(\mathbf{R},\mathbf{t}) &=& \sum_{i=1}^n || q_i - \mathbf{R}p_i - \mathbf{t} ||^2 \\
\end{eqnarray*}
To find the optimal translation $\mathbf{t}$, take the derivative wrt $\mathbf{t}$ and set equal to zero:
\begin{eqnarray*}
	\frac{\partial\mathcal{C}(\mathbf{R},\mathbf{t})}{\partial\mathbf{t}} &=& 0 \\
	\sum_{i=1}^n 2(q_i-\mathbf{R}p_i-\mathbf{t}) &=& 0\\
	\sum q_i - \sum \mathbf{R} p_i - \sum \mathbf{t} &=& 0 \\
	\sum q_i - \mathbf{R}\sum p_i &=& n\mathbf{t} \\
\end{eqnarray*}
Let $p$ and $q$ represent the centers of mass of the point sets  $p_i$ and $q_i$, respectively, such that:
$$p=\frac{1}{n}\sum_{i=1}^n p_i \textrm{ and } q=\frac{1}{n}\sum_{i=1}^n q_i$$
Substituting these sums in the above equation:
\begin{eqnarray*}
	n\mathbf{t} &=& nq - n\mathbf{R}p \\
	\mathbf{t} &=& q-\mathbf{R}p
\end{eqnarray*}
This gives a result for $\mathbf{t}$ in terms of $\mathbf{R}$.  Substituting for $\mathbf{t}$ 
\begin{eqnarray*}
	\mathcal{C}(\mathbf{R},\mathbf{t}) &=& 
		\sum_{i=1}^n || q_i - \mathbf{R}p_i - (q - \mathbf{R}p) ||^2 \\
	&=& \sum_{i=1}^n || ( q_i - q ) -\mathbf{R}(p_i - p) ||^2 \\
\end{eqnarray*}
Here, $(p_i-p)$ and $(q_i-q)$ are simply the recentered coordinates, call them $\tilde{p}_i$ and $\tilde{q}_i$.  Thus, the $\mathbf{R}$ and $\mathbf{t}$ that minimize $\mathcal{C}(\mathbf{R},\mathbf{t})$ are:
\begin{eqnarray*}
&&\mathbf{\hat{R}} \textrm{, which minimizes } 
	\sum_{i=1}^n || \tilde{q}_i - \mathbf{R}\tilde{p}_i ||^2 \\
&&\mathbf{t} = q-\mathbf{\hat{R}}p 
\end{eqnarray*}
Minimizing the first sum can be accomplished using any variety of techniques.  The simplest is perhaps the solution to the linear system $\mathbf{P \cdot r}=\mathbf{Q}$, where $\mathbf{r}$ is simply $\mathbf{R}$ reshaped to a $9\times 1$ matrix, and:
\begin{eqnarray*}
	\mathbf{P} &=& \left[
	\begin{array}{ccccccccc}
		\cdots \\
		x_i & y_i & z_i  &  0 & 0 & 0        &  0 & 0 & 0 \\
		0 & 0 & 0        &  x_i & y_i & z_i  &  0 & 0 & 0 \\
		0 & 0 & 0        &  0 & 0 & 0        &  x_i & y_i & z_i \\
		\cdots 
	\end{array}
	\right] \\
	\mathbf{Q} & = & \left[
	\begin{array}{c}
		\cdots \\
		x_i' \\
		y_i' \\
		z_i' \\
		\cdots
	\end{array}
	\right]
\end{eqnarray*} 
Here, $\tilde{p}_i=(x_i, y_i, z_i)$ and $\tilde{q}_i=(x_i', y_i', z_i')$.  The solution is $\mathbf{r}=\mathbf{P}^+\cdot\mathbf{Q}$, where $\mathbf{P}^+$ is the pseudo-inverse of $\mathbf{P}$.

If there are outliers in the data, a robust least-squares estimation scheme can be used such as
RANSAC or LMedS.
\bibliography{stanchak_enee731_final}
\end{document}
